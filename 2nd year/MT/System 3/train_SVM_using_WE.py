# -*- coding: utf-8 -*-
"""word_embeddings_features_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TYl0oefy32CF5NP_CCeewjUVbduINBhN
"""

import pandas as pd
import numpy as np
import re
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import gensim.downloader
import xgboost as xgb

from google.colab import drive
drive.mount('/content/drive/')

"""Read the train and test datasets and preprocess the lyrics using simple_preprocess function from gensim"""

df_train = pd.read_csv('/content/drive/MyDrive/Text Mining/dreaddit-train.csv')
df_test = pd.read_csv('/content/drive/MyDrive/Text Mining/dreaddit-test.csv')

train_text = df_train.text.values
clean_train_text = [gensim.utils.simple_preprocess(lyrics) for lyrics in train_text]
train_labels = df_train.label.values

test_text = df_test.text.values
clean_test_text = [gensim.utils.simple_preprocess(lyrics) for lyrics in test_text]
test_labels = df_test.label.values

"""Load pretrained glove gensim model"""

w2v = gensim.downloader.load('glove-twitter-200')

"""Method that computes the average word embedding for each document in a dataset"""

def generate_avg_word_embedding(documents):
  features = []
  for i, document in enumerate(documents):
    w_emb = []
    for word in document:
      if word in w2v.wv.index2word:
        w_emb.append(w2v[word])
    
    if len(w_emb) > 0:
      w_emb = np.array(w_emb)
      w_emb = np.mean(w_emb, axis=0)
    else:
      w_emb = np.zeros(200)
    
    features.append(w_emb)
  
  return np.array(features)

"""Compute word embeddings feature for both train and test"""

train_features = generate_avg_word_embedding(clean_train_text)

test_features = generate_avg_word_embedding(clean_test_text)

print(train_features.shape, test_features.shape)

"""Method that implements cross validation and returns the accuracy on train, validation and test"""

def cross_validation(X_train, y_train, X_test, y_test, model):
    accuracies = []
    kfold = KFold(n_splits=5)
    for i, (train_index, val_index) in enumerate(kfold.split(X_train)):
        train_lyr = X_train[train_index]
        train_gen = y_train[train_index]

        val_lyr = X_train[val_index]
        val_gen = y_train[val_index]

        model.fit(train_lyr, train_gen)

        predicted_train = model.predict(train_lyr)
        predicted_val = model.predict(val_lyr)
        predicted_test = model.predict(X_test)

        accuracies.append([accuracy_score(predicted_train, train_gen), accuracy_score(predicted_val, val_gen), accuracy_score(predicted_test, y_test)])

    
    return np.round(np.mean(np.array(accuracies), axis=0), 3)

"""# Support Vector Machine"""

C_params = [1, 5, 10]
kernel_params = ['linear', 'rbf']
decision_function_shape_params = ['ovo', 'ovr']
results = []

for C in C_params:
  for kernel in kernel_params:
    for decision_function_shape in decision_function_shape_params:
      model = SVC(C=C, kernel=kernel, decision_function_shape=decision_function_shape)
      accuracies = cross_validation(train_features, train_labels, test_features, test_labels, model)

      results.append([ C, kernel, decision_function_shape] + accuracies.tolist())

df_results_svm = pd.DataFrame(results, columns = ['C', 'kernel', 'decision_function_shape', 'train accuracy', 'cross-val accuracy', 'test accuracy'])
df_results_svm