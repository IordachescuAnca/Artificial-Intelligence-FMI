{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA5FK8qkLu-z",
        "outputId": "ae5b8fdd-e21d-4bd6-e585-c026ad361130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import nltk.corpus\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from itertools import combinations\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MltGRk6uL6Np",
        "outputId": "2911f5e6-03e7-4379-fccd-6bea67db2a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the word2vec"
      ],
      "metadata": {
        "id": "Jpp1_ER8WijH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset can be found here: http://jmcauley.ucsd.edu/data/amazon/"
      ],
      "metadata": {
        "id": "99aEeuUyVhhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read the json that contains the dataset\n",
        "data = pd.read_json('/content/gdrive/MyDrive/NLP/proiect2/reviews_Sports_and_Outdoors_5.json', lines=True)"
      ],
      "metadata": {
        "id": "YUq7ZrSsL7hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(data):\n",
        "  #get the preprocessing sentences from the dataset\n",
        "  tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "  #get only the data in the reviewText column\n",
        "  data = data['reviewText']\n",
        "  corpus = ''.join(data + \" \")\n",
        "\n",
        "  #tokenize the sentences\n",
        "  sentences = tokenizer.tokenize(corpus)\n",
        "  modified = []\n",
        "  for sentence in sentences:\n",
        "    #perform preprocessing in order to obtain only the words that contains letters\n",
        "    sentence = re.sub(re.compile(\"[^A-Za-z]+\"), \" \", sentence)\n",
        "    modified.append(sentence.strip().split())\n",
        "  \n",
        "  return modified\n",
        "\n",
        "documents = get_sentences(data)"
      ],
      "metadata": {
        "id": "wSBjgy9mQPIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build the vocabulary and train the word2vec\n",
        "model = Word2Vec(size=300, min_count=3, window=7, sg=1, seed=1)\n",
        "model.build_vocab(sentences=documents)\n",
        "model.train(sentences=documents, total_examples=model.corpus_count, epochs = 1)\n",
        "voc = model.wv.vocab\n",
        "print(\"Tokens: \", len(voc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kon8KyYRM-RX",
        "outputId": "8b335044-92dd-4bdb-d98a-614525bd9697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  54665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/gdrive/MyDrive/NLP/proiect2/word2vec_model.model\")"
      ],
      "metadata": {
        "id": "-7GzHNOPWWE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples for word2vec in order to see the most similar words for one entry"
      ],
      "metadata": {
        "id": "xWE-HtirV5cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(\"great\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2M7o-5DOGQN",
        "outputId": "5791c9a6-377c-47fc-e25b-5957f483bbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fantastic', 0.844652533531189),\n",
              " ('wonderful', 0.8247833251953125),\n",
              " ('terrific', 0.7980945110321045),\n",
              " ('fabulous', 0.7573331594467163),\n",
              " ('phenomenal', 0.7482106685638428),\n",
              " ('awesome', 0.739443302154541),\n",
              " ('Fantastic', 0.7390926480293274),\n",
              " ('good', 0.7317873239517212),\n",
              " ('prefect', 0.7303392887115479),\n",
              " ('excellent', 0.7212550044059753)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(\"cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5G_CpjbPR08",
        "outputId": "cfb5776c-890f-4fac-da54-e2bab396c035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('squirrel', 0.7444180846214294),\n",
              " ('daddy', 0.7386866807937622),\n",
              " ('cats', 0.7356756329536438),\n",
              " ('kiddie', 0.7353200912475586),\n",
              " ('bird', 0.7261258363723755),\n",
              " ('plink', 0.7246809005737305),\n",
              " ('coyotes', 0.7220070958137512),\n",
              " ('neighbor', 0.7194820642471313),\n",
              " ('lacrosse', 0.7177393436431885),\n",
              " ('nerf', 0.7172856330871582)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Word2vec"
      ],
      "metadata": {
        "id": "kEE2rna4W7tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load(\"/content/gdrive/MyDrive/NLP/proiect2/word2vec_model.model\")"
      ],
      "metadata": {
        "id": "gPvVHoFiXafP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coverage(model):\n",
        "  #check the percentage of the coverage\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  voc = model.wv.vocab\n",
        "  voc_wn = 0\n",
        "  voc_all = 0\n",
        "  words_not_wn = []\n",
        "  for key in voc:\n",
        "    voc_all += 1\n",
        "    #lemmatize the word of the vocabulary\n",
        "    word = lemmatizer.lemmatize(key)\n",
        "    synset_word = wn.synsets(key)\n",
        "    synset_lemma = wn.synsets(word)\n",
        "    #check if the word or its lemma has at least one synset\n",
        "    if len(synset_word) > 0 or len(synset_lemma) > 0:\n",
        "      voc_wn += 1\n",
        "    else:\n",
        "      words_not_wn.append(key)\n",
        "  \n",
        "  #print a bunch of examples of words that do not appear in the wordnet\n",
        "  print(\"Examples of words that are not in WN, but are in the vocabulary: {}\".format(words_not_wn[-10:-5]))\n",
        "  return voc_wn / voc_all\n"
      ],
      "metadata": {
        "id": "kNuMi95xXe2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(model):\n",
        "  #function to generate the sample of number_samples words from the vocabulary that are not stopwords\n",
        "  number_samples = 1000\n",
        "  voc = model.wv.vocab\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  k = 0\n",
        "  words = []\n",
        "  for key in voc:\n",
        "    if key.lower() not in stop_words:\n",
        "      words.append(key)\n",
        "      k += 1\n",
        "    \n",
        "    if k == number_samples:\n",
        "      break\n",
        "  \n",
        "  return words"
      ],
      "metadata": {
        "id": "J6dYuCeWXxm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def synonyms_emb(word1, word2, model, threshold):\n",
        "  #function to check if two words are similar based on cosine similarity and having a threshold\n",
        "  sim = model.wv.similarity(word1, word2)\n",
        "  if sim >= threshold:\n",
        "    return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "qRk0pM3fX6oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def synonyms_wn(word1, word2):\n",
        "  #function to check if two words have in common at least one synset\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  word1 = lemmatizer.lemmatize(word1)\n",
        "  word2 = lemmatizer.lemmatize(word2)\n",
        "  synset1 = set(wn.synsets(word1))\n",
        "  synset2 = set(wn.synsets(word2))\n",
        "  if len(synset1.intersection(synset2)) > 0:\n",
        "    return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "kllNOi0bYBv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate the words\n",
        "words = generate_sample(model)"
      ],
      "metadata": {
        "id": "shipxlmVYHZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds = [0.6, 0.7, 0.8, 0.9]\n",
        "#try various thresholds in order to obtain the best f1 score\n",
        "for threshold in thresholds:\n",
        "  #generate unique pairs of words\n",
        "  pairs = combinations(words, 2)\n",
        "  syn_emb = 0\n",
        "  syn_wn = 0\n",
        "  common = 0\n",
        "\n",
        "  for pair in pairs:\n",
        "    is_syn_emb = synonyms_emb(pair[0], pair[1], model, threshold)\n",
        "    if is_syn_emb:\n",
        "      #check if they are \"synonyms\" in terms of embeddings\n",
        "      syn_emb += 1\n",
        "\n",
        "    is_syn_wn = synonyms_wn(pair[0], pair[1])\n",
        "    if is_syn_wn:\n",
        "      #check if they are \"synonyms\" in terms of wordnet\n",
        "      syn_wn += 1\n",
        "\n",
        "    if is_syn_emb and is_syn_wn:\n",
        "      #check if they are \"synonyms\" in terms of embeddings and wornet\n",
        "      common += 1\n",
        "\n",
        "\n",
        "  #compute the metrics\n",
        "  precision = common / syn_emb\n",
        "  recall = common / syn_wn\n",
        "  f1 = 2 * precision * recall / (precision + recall)\n",
        "  print(\"Threshold: {} - Precision: {} - Recall: {} - F1: {}\".format(threshold, precision, recall, f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ2M8gPIYL16",
        "outputId": "bc4313fc-1c23-45c1-aa84-67e26bae937d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.6 - Precision: 0.019661762683899353 - Recall: 0.15294117647058825 - F1: 0.034844054580896684\n",
            "Threshold: 0.7 - Precision: 0.02579957356076759 - Recall: 0.06470588235294118 - F1: 0.03689024390243903\n",
            "Threshold: 0.8 - Precision: 0.01312910284463895 - Recall: 0.012834224598930482 - F1: 0.012979989183342347\n",
            "Threshold: 0.9 - Precision: 0.0033333333333333335 - Recall: 0.0010695187165775401 - F1: 0.001619433198380567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be seen above, the best f1 score is obtained using threshold = 0.7 . I am going to use this threshold below in order to compute the metrics and show the results for this assigment.\n",
        "\n",
        "Description:\n",
        "\n",
        "Chosen corpus: Amazon Product Data - Sports and Outdoors\n",
        "\n",
        "Language: English\n",
        "\n",
        "Number of tokens: 54665\n",
        "\n",
        "Coverage: 0.7258209091740602\n",
        "\n",
        "Precision: 0.02579957356076759\n",
        "\n",
        "Recall: 0.06470588235294118\n",
        "\n",
        "F1: 0.03689024390243903\n",
        "\n",
        "Threshold: 0.7"
      ],
      "metadata": {
        "id": "ZaMjtub7XZq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cov = coverage(model)\n",
        "print('Coverage: {}'.format(cov))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI9M1Uk4Z3E-",
        "outputId": "8bf2fb21-b0d4-431a-a2e6-55d2531aef96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples of words that are not in WN, but are in the vocabulary: ['Medifast', 'BlenderBall', 'Omaker', 'OnCore', 'Gogogu']\n",
            "Coverage: 0.7258209091740602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = combinations(words, 2)\n",
        "\n",
        "threshold = 0.7\n",
        "syn_emb = 0\n",
        "syn_wn = 0\n",
        "common = 0\n",
        "\n",
        "pair_emb = []\n",
        "pair_wn = []\n",
        "\n",
        "#the same code as above\n",
        "for pair in pairs:\n",
        "  is_syn_emb = synonyms_emb(pair[0], pair[1], model, threshold)\n",
        "  if is_syn_emb:\n",
        "    syn_emb += 1\n",
        "\n",
        "  is_syn_wn = synonyms_wn(pair[0], pair[1])\n",
        "  if is_syn_wn:\n",
        "    syn_wn += 1\n",
        "\n",
        "  if is_syn_emb and is_syn_wn:\n",
        "    common += 1\n",
        "  \n",
        "  if is_syn_emb and not is_syn_wn:\n",
        "    pair_emb.append(pair)\n",
        "  \n",
        "  if not is_syn_emb and is_syn_wn:\n",
        "    pair_wn.append(pair)\n",
        "\n",
        "\n",
        "precision = common / syn_emb\n",
        "recall = common / syn_wn\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "#show some precision and recall errors\n",
        "print(\"Threshold: {} - Precision: {} - Recall: {} - F1: {}\".format(threshold, precision, recall, f1))\n",
        "print(\"Precision errors(word pairs synonyms in the embedding space, but not in the same synset in WN): {}\".format(pair_emb[5:10]))\n",
        "print(\"Recall errors(word pairs not synonyms in the embedding space, but in the same synset in WN): {}\".format(pair_wn[5:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQGwoB1nZrs9",
        "outputId": "4f24a140-01f8-43a4-e1bd-2182819c5720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.7 - Precision: 0.02579957356076759 - Recall: 0.06470588235294118 - F1: 0.03689024390243903\n",
            "Precision errors(word pairs synonyms in the embedding space, but not in the same synset in WN): [('haved', 'Bit'), ('haved', 'Assembling'), ('haved', 'mice'), ('haved', 'Bounced'), ('haved', 'witch')]\n",
            "Recall errors(word pairs not synonyms in the embedding space, but in the same synset in WN): [('came', 'got'), ('came', 'amount'), ('came', 'amounts'), ('came', 'number'), ('came', 'Comes')]\n"
          ]
        }
      ]
    }
  ]
}