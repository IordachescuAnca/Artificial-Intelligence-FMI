{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Sense Disambiguation using Supervised Learning\n",
        "## The Naive Bayes Model\n",
        "\n"
      ],
      "metadata": {
        "id": "f9Gcv1yBJ8J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Senseval dataset"
      ],
      "metadata": {
        "id": "GVIk2jywPlZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Senseval 2 corpus is a word sense disambiguation corpus. Each item in the corpus corresponds to a single ambiguous word. For each of these words, the corpus contains a list of instances, corresponding to occurrences of that word. Each instance provides the word; a list of word senses that apply to the word occurrence; and the wordâ€™s context.\n",
        "https://www.nltk.org/howto/corpus.html#senseval\n",
        "\n",
        "Detailed description of dataset creation in publication here: https://aclanthology.org/S01-1001/"
      ],
      "metadata": {
        "id": "ShaWqTAEQM3_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAlCV_904PBQ",
        "outputId": "52f1c245-5d2f-4367-9665-9e761505cba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]   Package senseval is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('senseval')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import senseval\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk import tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inst = senseval.instances('interest.pos')\n",
        "inst[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDw_wJRsnUqb",
        "outputId": "95214074-47d9-43df-a0ce-8b14cd1b0c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SensevalInstance(word='interest-n', position=18, context=[('yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ('mutual', 'JJ'), ('funds', 'NNS'), ('continued', 'VBD'), ('to', 'TO'), ('slide', 'VB'), (',', ','), ('amid', 'IN'), ('signs', 'VBZ'), ('that', 'IN'), ('portfolio', 'NN'), ('managers', 'NNS'), ('expect', 'VBP'), ('further', 'JJ'), ('declines', 'NNS'), ('in', 'IN'), ('interest', 'NN'), ('rates', 'NNS'), ('.', '.')], senses=('interest_6',))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(inst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WahiHkhxJ5Ui",
        "outputId": "13301984-7a30-4b41-c676-bbde03173177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2368"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inst in senseval.instances('interest.pos')[:40]:\n",
        "  p = inst.position\n",
        "  left = ' '.join(w for (w,t) in inst.context[p-3:p])\n",
        "  word = ' '.join(w for (w,t) in inst.context[p:p+1])\n",
        "  right = ' '.join(w for (w,t) in inst.context[p+1:p+4])\n",
        "  senses = ' '.join(inst.senses)\n",
        "  print('%20s |%10s | %-15s -> %s' % (left, word, right, senses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUoElExrPoe8",
        "outputId": "aff6119a-f3ec-47ef-fde3-722127509012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " further declines in |  interest | rates .         -> interest_6\n",
            "to indicate declining |  interest | rates because they -> interest_6\n",
            " rises in short-term |  interest | rates .         -> interest_6\n",
            "               . 4 % |  interest | in this energy-services -> interest_5\n",
            "holding company with | interests | in the mechanical -> interest_5\n",
            "     refunded , plus |  interest | .               -> interest_6\n",
            "       curry set the |  interest | rate on the     -> interest_6\n",
            "      country 's own |  interest | , prompted the  -> interest_4\n",
            "    of principal and |  interest | is the only     -> interest_6\n",
            "     to increase its |  interest | to 70 %         -> interest_5\n",
            "     show the strong |  interest | of japanese investors -> interest_1\n",
            "    retired early if |  interest | rates decline , -> interest_6\n",
            "         the drop in |  interest | rates since the -> interest_6\n",
            "         the drop in |  interest | rates eventually will -> interest_6\n",
            "    par plus accrued |  interest | to the date     -> interest_6\n",
            "dramatic fluctuation in |  interest | rates , the     -> interest_6\n",
            "by growing international |  interest | in japanese behavior -> interest_1\n",
            "               . s . |  interest | rates has spurred -> interest_6\n",
            " again showed little |  interest | in further evidence -> interest_1\n",
            "   to cut short-term |  interest | rates , but     -> interest_6\n",
            "   key federal funds |  interest | rate by about   -> interest_6\n",
            " resulting in higher |  interest | costs to the    -> interest_6\n",
            "those japanese business | interests | { in the        -> interest_3\n",
            "                     |  interest | in american small -> interest_1\n",
            ", bought controlling |  interest | in the glass    -> interest_5\n",
            "        have a great |  interest | in making investments -> interest_1\n",
            "          holds 35 % |  interest | or more and     -> interest_5\n",
            "  primarily with the |  interest | rates they pay  -> interest_6\n",
            "  and foreign annual |  interest | rates below are -> interest_6\n",
            "64 million semiannual |  interest | payment due yesterday -> interest_6\n",
            "        a variety of |  interest | rate swap transactions -> interest_6\n",
            "  sterling market in |  interest | rate swap dealings -> interest_6\n",
            "        engage in an |  interest | rate swap ,     -> interest_6\n",
            "  to make fixed-rate |  interest | payments on debt -> interest_6\n",
            "                     |  interest | rates rose ,    -> interest_6\n",
            "    , noting growing |  interest | in use of       -> interest_1\n",
            "   them dealing with |  interest | rates and the   -> interest_6\n",
            "         bonds , the |  interest | on which is     -> interest_6\n",
            "fielded every important |  interest | on their team   -> interest_4\n",
            "protecting the national |  interest | in encouraging new -> interest_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "senseval.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y-I23ZHhbB9",
        "outputId": "9e74558f-8339-4f86-e878-b6f48bb64700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def senses(word):\n",
        "    \"\"\"\n",
        "    This takes a target word from senseval-2 (find out what the possible\n",
        "    are by running senseval.fileides()), and it returns the list of possible \n",
        "    senses for the word\n",
        "    \"\"\"\n",
        "    return list(set(i.senses[0] for i in senseval.instances(word)))\n",
        "\n",
        "senses('interest.pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJUXH24cPwLY",
        "outputId": "20a7c50a-88f9-4b28-8c78-944cd45f80de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['interest_3',\n",
              " 'interest_2',\n",
              " 'interest_1',\n",
              " 'interest_6',\n",
              " 'interest_5',\n",
              " 'interest_4']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in senseval.instances('interest.pos') if i.senses[0]=='interest_1']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "DpBAPChCX2Uy",
        "outputId": "cf41cdbc-44f1-451a-bf90-ef025bb2c670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d99da3505689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msenseval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'interest.pos'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'interest_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'senseval' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "senses('line.pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bkzmb2dHyTs",
        "outputId": "6196c3f0-162c-4327-d938-72e9064d7a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['division', 'text', 'cord', 'formation', 'phone', 'product']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "senses('serve.pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3M2-FheIb2_",
        "outputId": "a8ef3de9-aa2f-4d94-a8cd-fe0d1a42c019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SERVE12', 'SERVE10', 'SERVE6', 'SERVE2']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inst in senseval.instances('hard.pos')[0:10]:\n",
        "  p = inst.position\n",
        "  left = ' '.join(w for (w,t) in inst.context[p-3:p])\n",
        "  word = ' '.join(w for (w,t) in inst.context[p:p+1])\n",
        "  right = ' '.join(w for (w,t) in inst.context[p+1:p+4])\n",
        "  senses = ' '.join(inst.senses)\n",
        "  print('%20s |%10s | %-15s -> %s' % (left, word, right, senses))\n",
        "\n",
        "for inst in senseval.instances('hard.pos')[-10:]:\n",
        "  p = inst.position\n",
        "  left = ' '.join(w for (w,t) in inst.context[p-3:p])\n",
        "  word = ' '.join(w for (w,t) in inst.context[p:p+1])\n",
        "  right = ' '.join(w for (w,t) in inst.context[p+1:p+4])\n",
        "  senses = ' '.join(inst.senses)\n",
        "  print('%20s |%10s | %-15s -> %s' % (left, word, right, senses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR1nbaKqH-9W",
        "outputId": "814a30e1-0133-4cb6-cbb0-193d8c06a0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         and that 's |      hard | to do .         -> HARD1\n",
            "        are having a |      hard | time helping president -> HARD1\n",
            "           i find it |      hard | to believe that -> HARD1\n",
            "        person , the |      hard | part in correcting -> HARD1\n",
            "        'our life is |    harder | now , yes       -> HARD1\n",
            "   which have become |      hard | to sell in      -> HARD1\n",
            "         to face the |      hard | facts of life   -> HARD1\n",
            "                     |      hard | to make the     -> HARD1\n",
            "           it may be |      hard | just to finish  -> HARD1\n",
            "             , is it |      hard | portraying matt dillon -> HARD1\n",
            "       also weed out |      hard | cover classics that -> HARD3\n",
            "        cushion on a |      hard | headboard for comfortable -> HARD3\n",
            "            a rock - |      hard | field against the -> HARD3\n",
            "   either fabrics or |      hard | surfaces .      -> HARD3\n",
            "        ivory is the |      hard | endosperm of the -> HARD3\n",
            "     capitol was its |      hard | floors , the    -> HARD3\n",
            "                     |      hard | outer shell of  -> HARD3\n",
            "                     |    harder | skin will allow -> HARD3\n",
            "highway through this |      hard | scrabble state . -> HARD3\n",
            "       example : the |      hard | stick of the    -> HARD3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "senses('hard.pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPlzrqL1H5H7",
        "outputId": "10883283-b92f-4d00-9829-e303437c48a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HARD3', 'HARD2', 'HARD1']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Naive Bayes model"
      ],
      "metadata": {
        "id": "ctmzPkOGRCz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Bayes's classifier in order to label (classify) the words with e certain WordNet sense. For this we need a context window surrounding the target word (the word for which we search the sense). The context window should contain only \"content words\" (words with important meaning, that bring information, like nouns, verbs etc)\n",
        "\n",
        "We note P(s|c) the probability for sense s in the context c. For each such sense of the target word the probability is computed and we take the sense with the highest probability compared to the others.\n",
        "\n",
        "In order to compute the probability `P(s|c)`, we use the formula: \n",
        "\n",
        "`P(s|c)=P(c|s)*P(s)/P(c)`. \n",
        "\n",
        "`P(s)` is the probability of a sense without any context. For computing `P(c|s)` we need a training set (with texts that contain the target word, already labeled with its correct sense)."
      ],
      "metadata": {
        "id": "69h-hHDYRFPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK already has the classifier implemented. In this laboratory we will use the NLTK NaiveBayesClassifier:https://www.nltk.org/_modules/nltk/classify/naivebayes.html\n",
        "\n",
        "The Naive Bayes classifier will first compute the prior probability for the senses (or, generally speaking, for the class labels) - this is determined by the label's frequncy in the training set. the features are used to see the likelyhood of having that label in a given context."
      ],
      "metadata": {
        "id": "tZe-mCmaRSno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.classify import accuracy, NaiveBayesClassifier, MaxentClassifier\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "dUT9Ix0clUWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "Th1mp9VuleWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "where `train_set` must contain a list with the classes and features for each class. The train_set list will contain tuples of two elements. First element is a dictionary with the features (name and value of each feature). The second element is the class label.\n"
      ],
      "metadata": {
        "id": "BQlyxjDJRiCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sense_instances(instances, sense):\n",
        "    \"\"\"\n",
        "    This returns the list of instances in instances that have the sense sense\n",
        "    \"\"\"\n",
        "    return [instance for instance in instances if instance.senses[0]==sense]"
      ],
      "metadata": {
        "id": "MGdeKgbWRVQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sense2 = sense_instances(senseval.instances('hard.pos'), 'HARD2')"
      ],
      "metadata": {
        "id": "VArRTSpWl_Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sense2[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlXslxYTmMCN",
        "outputId": "18dbcf48-c291-44ea-c7c2-1937a7cfe4cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SensevalInstance(word='hard-a', position=15, context=[('keep', 'VB'), ('this', 'DT'), ('one', 'CD'), ('in', 'IN'), ('your', 'PRP$'), ('drawer', 'NN'), ('for', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('time', 'NN'), ('the', 'DT'), ('boss', 'NN'), ('gives', 'VBZ'), ('you', 'PRP'), ('a', 'DT'), ('hard', 'JJ'), ('time', 'NN'), ('.', '.')], senses=('HARD2',)),\n",
              " SensevalInstance(word='hard-a', position=11, context=[('she', 'PRP'), ('recommends', 'VBZ'), ('continuing', 'VBG'), ('education', 'NN'), ('courses', 'NNS'), (',', ','), ('developing', 'VBG'), ('effective', 'JJ'), ('people', 'NNS'), ('skills', 'NNS'), ('and', 'CC'), ('hard', 'JJ'), ('work', 'NN'), ('.', '.')], senses=('HARD2',)),\n",
              " SensevalInstance(word='hard-a', position=10, context=[('the', 'DT'), ('phrase', 'NN'), ('``', '``'), ('consent', 'NN'), ('of', 'IN'), ('the', 'DT'), ('governed', 'VBN'), (\"''\", \"''\"), ('needs', 'VBZ'), ('a', 'DT'), ('hard', 'JJ'), ('look', 'NN'), ('.', '.')], senses=('HARD2',)),\n",
              " SensevalInstance(word='hard-a', position=9, context=[('he', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('friend', 'NN'), (',', ','), ('and', 'CC'), ('hopefully', 'RB'), ('the', 'DT'), ('hard', 'JJ'), ('work', 'NN'), ('he', 'PRP'), ('has', 'VBZ'), ('done', 'VBN'), ('on', 'IN'), ('his', 'PRP$'), ('swing', 'NN'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('pay', 'VB'), ('dividends', 'NNS'), ('for', 'IN'), ('him', 'PRP'), ('.', '.')], senses=('HARD2',)),\n",
              " SensevalInstance(word='hard-a', position=39, context=[('``', '``'), ('what', 'WP'), ('we', 'PRP'), ('do', 'VBP'), ('is', 'VBZ'), ('really', 'RB'), ('show', 'VBP'), ('the', 'DT'), ('slapstick', 'NN'), ('side', 'NN'), ('that', 'WDT'), ('still', 'RB'), ('works', 'VBZ'), ('with', 'IN'), ('the', 'DT'), ('interpretation', 'NN'), ('(', '('), ('of', 'IN'), ('the', 'DT'), ('play', 'NN'), (')', 'SYM'), (',', ','), ('so', 'RB'), ('kids', 'NNS'), ('see', 'VBP'), ('how', 'WRB'), ('much', 'JJ'), ('fun', 'NN'), ('shakespeare', 'NNP'), ('is', 'VBZ'), ('--', ':'), ('because', 'IN'), ('he', 'PRP'), (\"'s\", 'VBZ'), ('(', '('), ('often', 'RB'), (')', 'SYM'), ('associated', 'VBN'), ('with', 'IN'), ('hard', 'JJ'), ('academic', 'JJ'), ('disciplines', 'NNS'), (',', ','), ('\"', '\"'), ('orlando', 'NNP'), ('said', 'VBD'), ('.', '.')], senses=('HARD2',))]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "STOPWORDS_SET = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC1-ShGfuZV2",
        "outputId": "9361bf28-0f7a-413a-8640-08517056a3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some helper functions we'll need to train our model\n",
        "\n",
        "def extract_vocab_frequency(instances, stopwords=STOPWORDS_SET, n=300):\n",
        "    \"\"\"\n",
        "    Given a list of senseval instances, return a list of the n most frequent words that\n",
        "    appears in its context (i.e., the sentence with the target word in), output is in order\n",
        "    of frequency and includes also the number of instances in which that key appears in the\n",
        "    context of instances.\n",
        "    \"\"\"\n",
        "    fd = nltk.FreqDist()\n",
        "    for i in instances:\n",
        "        (target, suffix) = i.word.split('-')\n",
        "        words = (c[0] for c in i.context if not c[0] == target)\n",
        "        for word in set(words) - set(stopwords):\n",
        "            fd[word] += 1\n",
        "    return fd.most_common()[:n+1]"
      ],
      "metadata": {
        "id": "gbVdHnBEnt0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vocab(instances, stopwords=STOPWORDS_SET, n=300):\n",
        "    return [w for w,f in extract_vocab_frequency(instances,stopwords,n)]"
      ],
      "metadata": {
        "id": "zYxtcImpuiWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vocab_frequency2(instances, word, stopwords=STOPWORDS_SET, n=300):\n",
        "  fd = nltk.FreqDist()\n",
        "  for i in instances:\n",
        "    remained_words = set(i) - set(stopwords) - set(word)\n",
        "    for word in set(i) - remained_words:\n",
        "      fd[word] += 1\n",
        "\n",
        "  return fd.most_common()[:n+1]\n",
        "\n",
        "\n",
        "def extract_vocab2(sentences, word, stopwords=STOPWORDS_SET, n=300):\n",
        "  return [w for w, f in extract_vocab_frequency2(sentences,word,stopwords,n)]"
      ],
      "metadata": {
        "id": "mVInqkNtZZUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_vocab(senseval.instances('interest.pos'), stopwords=STOPWORDS_SET, n=1000)[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYOgC8-6CZJ0",
        "outputId": "2bbf2b73-987f-462d-fc0c-43653bc113ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.',\n",
              " ',',\n",
              " 'rates',\n",
              " \"'s\",\n",
              " 'said',\n",
              " '%',\n",
              " 'interests',\n",
              " '``',\n",
              " \"''\",\n",
              " '$',\n",
              " 'million',\n",
              " 'n',\n",
              " \"'t\",\n",
              " 'mr',\n",
              " 'company',\n",
              " 'u',\n",
              " 'rate',\n",
              " 'would',\n",
              " 'market',\n",
              " 'bonds']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction\n",
        "\n",
        "def wsd_context_features(instance, vocab, dist=3):\n",
        "    features = {}\n",
        "    ind = instance.position\n",
        "    con = instance.context\n",
        "    for i in range(max(0, ind-dist), ind):\n",
        "        j = ind-i\n",
        "        features['left-context-word-%s(%s)' % (j, con[i][0])] = True\n",
        "\n",
        "    for i in range(ind+1, min(ind+dist+1, len(con))):\n",
        "        j = i-ind\n",
        "        features['right-context-word-%s(%s)' % (j, con[i][0])] = True\n",
        "\n",
        " \n",
        "    features['word'] = instance.word\n",
        "    features['pos'] = con[1][1]\n",
        "    return features"
      ],
      "metadata": {
        "id": "xf6qTo6EmQ3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This feature set represents the context of a word w as the sequence of m pairs (word,tag) that occur before w and the sequence of m pairs (word, tag) that occur after w. As we'll see shortly, you can specify the value of m (e.g., m=1 means the context consists of just the immediately prior and immediately subsequent word-tag pairs); otherwise, m defaults to 3."
      ],
      "metadata": {
        "id": "bNZsTB2VC-tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "senseval.instances('interest.pos')[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fUJ2LlhAbYh",
        "outputId": "dbd1526e-90a6-47b2-c362-3b701010e516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SensevalInstance(word='interest-n', position=8, context=[('finmeccanica', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('italian', 'NN'), ('state-owned', 'JJ'), ('holding', 'NN'), ('company', 'NN'), ('with', 'IN'), ('interests', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('mechanical', 'JJ'), ('engineering', 'NN'), ('industry', 'NN'), ('.', '.')], senses=('interest_5',))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_interest = extract_vocab(senseval.instances('interest.pos'), stopwords=[], n=300)\n",
        "wsd_context_features(senseval.instances('interest.pos')[4], vocab=vocab_interest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1YnrWt5_Lgx",
        "outputId": "9fe379c2-846d-4fec-9c44-87f8ae365ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'left-context-word-1(with)': True,\n",
              " 'left-context-word-2(company)': True,\n",
              " 'left-context-word-3(holding)': True,\n",
              " 'pos': 'VBZ',\n",
              " 'right-context-word-1(in)': True,\n",
              " 'right-context-word-2(the)': True,\n",
              " 'right-context-word-3(mechanical)': True,\n",
              " 'word': 'interest-n'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wsd_word_features(instance, vocab, dist=3):\n",
        "    \"\"\"\n",
        "    Create a featureset where every key returns False unless it occurs in the\n",
        "    instance's context\n",
        "    \"\"\"\n",
        "    features = defaultdict(lambda:False)\n",
        "    features['alwayson'] = True\n",
        "    #cur_words = [w for (w, pos) in i.context]\n",
        "    try:\n",
        "      # \n",
        "        for(w, pos) in instance.context:\n",
        "            if w in vocab:\n",
        "                features[w] = True\n",
        "    except ValueError:\n",
        "        pass\n",
        "    return features"
      ],
      "metadata": {
        "id": "dKHvAOqrnseh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wsd_context_features1(instance, word, vocab, dist=3):\n",
        "  #try:\n",
        "  #  ind = instance.index(word)\n",
        "  #except ValueError:\n",
        "  #  return {}\n",
        "  ind = instance.index(word)\n",
        "  features = {}\n",
        "  for i in range(max(0, ind - dist), ind):\n",
        "    j = ind-i\n",
        "    features['left-context-word-%s(%s)' % (j, instance[i])] = True\n",
        "\n",
        "  for i in range(ind+1, min(ind+dist+1, len(instance))):\n",
        "    j = ind-i\n",
        "    features['right-context-word-%s(%s)' % (j, instance[i])] = True\n",
        "\n",
        "  features['word'] = word   \n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "1mf7G6HeZe3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This feature set is based on the set S of the n most frequent words that occur in the same sentence as the target word w across the entire training corpus (as you'll see later, you can specify the value of n, but if you don't specify it then it defaults to 300). For each occurrence of w, wsd_word_features represents its context as the subset of those words from S that occur in the w's sentence."
      ],
      "metadata": {
        "id": "_DUHzrD4DFCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "senseval.instances('interest.pos')[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-PCzbqHRglJ",
        "outputId": "73f9791a-eac7-40fa-b7e3-6730979ac3bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SensevalInstance(word='interest-n', position=8, context=[('finmeccanica', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('italian', 'NN'), ('state-owned', 'JJ'), ('holding', 'NN'), ('company', 'NN'), ('with', 'IN'), ('interests', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('mechanical', 'JJ'), ('engineering', 'NN'), ('industry', 'NN'), ('.', '.')], senses=('interest_5',))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wsd_word_features(senseval.instances('interest.pos')[4], vocab=vocab_interest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hh__rXGCvOO",
        "outputId": "f1200f61-e38f-4bf3-acbc-42aa409676c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.wsd_word_features.<locals>.<lambda>>,\n",
              "            {'.': True,\n",
              "             'alwayson': True,\n",
              "             'an': True,\n",
              "             'company': True,\n",
              "             'holding': True,\n",
              "             'in': True,\n",
              "             'industry': True,\n",
              "             'interests': True,\n",
              "             'is': True,\n",
              "             'the': True,\n",
              "             'with': True})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_inst_cache = {}\n"
      ],
      "metadata": {
        "id": "qCOvautC9it0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wsd_classifier(trainer, word, features, stopwords_list = STOPWORDS_SET, number=300, distance=3, confusion_matrix=False):\n",
        "    \"\"\"\n",
        "    This function takes as arguments:\n",
        "        a trainer (e.g., NaiveBayesClassifier.train);\n",
        "        a target word from senseval2\n",
        "        a feature set (this can be wsd_context_features or wsd_word_features);\n",
        "        a number (defaults to 300), which determines for wsd_word_features the number of\n",
        "            most frequent words within the context of a given sense that you use to classify examples;\n",
        "        a distance (defaults to 3) which determines the size of the window for wsd_context_features (if distance=3, then\n",
        "            wsd_context_features gives 3 words and tags to the left and 3 words and tags to\n",
        "            the right of the target word);\n",
        "        confusion_matrix (defaults to False), which if set to True prints a confusion matrix.\n",
        "\n",
        "    Calling this function splits the senseval data for the word into a training set and a test set (the way it does\n",
        "    this is the same for each call of this function, because the argument to random.seed is specified,\n",
        "    but removing this argument would make the training and testing sets different each time you build a classifier).\n",
        "\n",
        "    It then trains the trainer on the training set to create a classifier that performs WSD on the word,\n",
        "    using features (with number or distance where relevant).\n",
        "\n",
        "    It then tests the classifier on the test set, and prints its accuracy on that set.\n",
        "\n",
        "\n",
        "    If confusion_matrix==True, then calling this function prints out a confusion matrix, where each cell [i,j]\n",
        "    indicates how often label j was predicted when the correct label was i (so the diagonal entries indicate labels\n",
        "    that were correctly predicted).\n",
        "    \"\"\"\n",
        "    print(\"Reading data...\")\n",
        "    #global _inst_cache\n",
        "    if word not in _inst_cache:\n",
        "        _inst_cache[word] = [(i, i.senses[0]) for i in senseval.instances(word)]\n",
        "    events = _inst_cache[word][:]\n",
        "    senses = list(set(l for (i, l) in events))\n",
        "    instances = [i for (i, l) in events]\n",
        "    vocab = extract_vocab(instances, stopwords=stopwords_list, n=number)\n",
        "    print(' Senses: ' + ' '.join(senses))\n",
        "\n",
        "    # Split the instances into a training and test set,\n",
        "    #if n > len(events): n = len(events)\n",
        "    n = len(events)\n",
        "    random.seed(334)\n",
        "    random.shuffle(events)\n",
        "    training_data = events[:int(0.8 * n)]\n",
        "    test_data = events[int(0.8 * n):n]\n",
        "\n",
        "    # Train classifier\n",
        "    print('Training classifier...')\n",
        "    classifier = trainer([(features(i, vocab, distance), label) for (i, label) in training_data])\n",
        "    # Test classifier\n",
        "    print('Testing classifier...')\n",
        "    acc = accuracy(classifier, [(features(i, vocab, distance), label) for (i, label) in test_data] )\n",
        "    print('Accuracy: %6.4f' % acc)\n",
        "    \n",
        "    if confusion_matrix==True:\n",
        "        gold = [label for (i, label) in test_data]\n",
        "        derived = [classifier.classify(features(i,vocab)) for (i,label) in test_data]\n",
        "        cm = nltk.ConfusionMatrix(gold,derived)\n",
        "        print(cm)\n",
        "    \n",
        "    return classifier\n",
        "        "
      ],
      "metadata": {
        "id": "DKLXs_oF5dsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the classifier:\n",
        "# NB, with features based on 300 most frequent context words\n",
        "wsd_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features)\n",
        "\n",
        "# Pseudocode, general training steps:\n",
        "# featureset = [extract_features[i] for i in instances]\n",
        "# classifier = NaiveBayesClassifier.train((feature, label) for feature in featureset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swpzhD0A-SsJ",
        "outputId": "26124cf0-0140-4814-f940-b03574b24e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            " Senses: HARD3 HARD2 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NB, with features based word + pos in 6 word window\n",
        "wsd_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_context_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DFxCrwF-qG7",
        "outputId": "9c65ffd7-6a81-4e39-8be7-253e0ecd3ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            " Senses: HARD3 HARD2 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wsd_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_context_features, confusion_matrix=True) # 0.33"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM_cVe_d-vSo",
        "outputId": "f28aaaea-7889-476b-f2ac-fd8f9e87cad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            " Senses: HARD3 HARD2 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8927\n",
            "      |   H   H   H |\n",
            "      |   A   A   A |\n",
            "      |   R   R   R |\n",
            "      |   D   D   D |\n",
            "      |   1   2   3 |\n",
            "------+-------------+\n",
            "HARD1 |<650> 35  16 |\n",
            "HARD2 |  12 <76>  5 |\n",
            "HARD3 |   7  18 <48>|\n",
            "------+-------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wsd_classifier(NaiveBayesClassifier.train, 'interest.pos', wsd_context_features, confusion_matrix=True) # 1/6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpwwVJpLaGci",
        "outputId": "2a72a4e8-3066-43ab-f2fc-44c9d81ad311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            " Senses: interest_5 interest_4 interest_2 interest_1 interest_3 interest_6\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.4219\n",
            "           |   i   i   i   i   i   i |\n",
            "           |   n   n   n   n   n   n |\n",
            "           |   t   t   t   t   t   t |\n",
            "           |   e   e   e   e   e   e |\n",
            "           |   r   r   r   r   r   r |\n",
            "           |   e   e   e   e   e   e |\n",
            "           |   s   s   s   s   s   s |\n",
            "           |   t   t   t   t   t   t |\n",
            "           |   _   _   _   _   _   _ |\n",
            "           |   1   2   3   4   5   6 |\n",
            "-----------+-------------------------+\n",
            "interest_1 | <24> 33   4   4   5   1 |\n",
            "interest_2 |   .  <3>  .   1   .   . |\n",
            "interest_3 |   2   2  <4>  .   .   . |\n",
            "interest_4 |   1  22   2 <14>  .   2 |\n",
            "interest_5 |   5  45   1   6 <61>  2 |\n",
            "interest_6 |   . 133   2   1   . <94>|\n",
            "-----------+-------------------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wsd_classifier(NaiveBayesClassifier.train, 'interest.pos', wsd_context_features) # 1/6 ~ 0.16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvC5-r8I-4MR",
        "outputId": "2ee7fad5-d9a1-4d04-861f-ccfdbcc367e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            " Senses: interest_5 interest_4 interest_2 interest_1 interest_3 interest_6\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.4219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is the accuracy lower for \"interest\"...?"
      ],
      "metadata": {
        "id": "1IshPIx4_aQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline**: how could we guess the sense of a word without any additional information?"
      ],
      "metadata": {
        "id": "t6Azd3gK_YBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Baseline\n",
        "hard_sense_fd = nltk.FreqDist([i.senses[0] for i in senseval.instances('hard.pos')])\n",
        "most_frequent_hard_sense= list(hard_sense_fd.keys())[0]\n",
        "frequency_hard_sense_baseline = hard_sense_fd.freq(list(hard_sense_fd.keys())[0])\n"
      ],
      "metadata": {
        "id": "8YAdO3sC_Xk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_hard_sense_baseline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo8YHepH_4De",
        "outputId": "5a1d0f9f-8712-4171-e0f6-c0e78fb43987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.797369028386799"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interest_sense_fd = nltk.FreqDist([i.senses[0] for i in senseval.instances('interest.pos')])\n",
        "most_frequent_interest_sense= list(interest_sense_fd.keys())[0]\n",
        "frequency_interest_sense_baseline = interest_sense_fd.freq(list(interest_sense_fd.keys())[0])"
      ],
      "metadata": {
        "id": "iqI6Vlk0_6j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_interest_sense_baseline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37g-BJ70ACiN",
        "outputId": "8785d821-fd36-4896-9fb2-376ea83f1c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5287162162162162"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use Naive Bayes classifier from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
      ],
      "metadata": {
        "id": "hwKruFv5ldFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "foqEoeItxFeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_report??"
      ],
      "metadata": {
        "id": "d9bU8BYGxQh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercitii (1p)"
      ],
      "metadata": {
        "id": "ovgwGIJ6fYH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a sample of the iWeb corpus, available here: https://www.corpusdata.org/iweb/samples/text0.zip . Unzip the archive and choose on of the text files in the archive at random. You will use it in the next exercises."
      ],
      "metadata": {
        "id": "aqG5R8bkjv16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.corpusdata.org/iweb/samples/text0.zip"
      ],
      "metadata": {
        "id": "xvtKXGiPkKCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0994cc3c-84a8-4c06-8a02-18999be02d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 17:51:11--  https://www.corpusdata.org/iweb/samples/text0.zip\n",
            "Resolving www.corpusdata.org (www.corpusdata.org)... 209.90.108.238\n",
            "Connecting to www.corpusdata.org (www.corpusdata.org)|209.90.108.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37647973 (36M) [application/x-zip-compressed]\n",
            "Saving to: â€˜text0.zipâ€™\n",
            "\n",
            "text0.zip           100%[===================>]  35.90M   934KB/s    in 40s     \n",
            "\n",
            "2022-05-25 17:51:51 (930 KB/s) - â€˜text0.zipâ€™ saved [37647973/37647973]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -x 'text0.zip'"
      ],
      "metadata": {
        "id": "sNOdNjAHkL-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8238ce9-d36b-4fbf-d476-33899b6ff89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  text0.zip\n",
            "  inflating: 103053.txt              \n",
            "  inflating: 116053.txt              \n",
            "  inflating: 12053.txt               \n",
            "  inflating: 128053.txt              \n",
            "  inflating: 138053.txt              \n",
            "  inflating: 140053.txt              \n",
            "  inflating: 152053.txt              \n",
            "  inflating: 161053.txt              \n",
            "  inflating: 167053.txt              \n",
            "  inflating: 17053.txt               \n",
            "  inflating: 179053.txt              \n",
            "  inflating: 181053.txt              \n",
            "  inflating: 183053.txt              \n",
            "  inflating: 185053.txt              \n",
            "  inflating: 19053.txt               \n",
            "  inflating: 206053.txt              \n",
            "  inflating: 228053.txt              \n",
            "  inflating: 241053.txt              \n",
            "  inflating: 246053.txt              \n",
            "  inflating: 247053.txt              \n",
            "  inflating: 253053.txt              \n",
            "  inflating: 259053.txt              \n",
            "  inflating: 27053.txt               \n",
            "  inflating: 273053.txt              \n",
            "  inflating: 287053.txt              \n",
            "  inflating: 297053.txt              \n",
            "  inflating: 298053.txt              \n",
            "  inflating: 305053.txt              \n",
            "  inflating: 313053.txt              \n",
            "  inflating: 328053.txt              \n",
            "  inflating: 33053.txt               \n",
            "  inflating: 338053.txt              \n",
            "  inflating: 341053.txt              \n",
            "  inflating: 343053.txt              \n",
            "  inflating: 344053.txt              \n",
            "  inflating: 355053.txt              \n",
            "  inflating: 357053.txt              \n",
            "  inflating: 361053.txt              \n",
            "  inflating: 363053.txt              \n",
            "  inflating: 366053.txt              \n",
            "  inflating: 370053.txt              \n",
            "  inflating: 372053.txt              \n",
            "  inflating: 375053.txt              \n",
            "  inflating: 376053.txt              \n",
            "  inflating: 380053.txt              \n",
            "  inflating: 384053.txt              \n",
            "  inflating: 391053.txt              \n",
            "  inflating: 406053.txt              \n",
            "  inflating: 407053.txt              \n",
            "  inflating: 409053.txt              \n",
            "  inflating: 426053.txt              \n",
            "  inflating: 427053.txt              \n",
            "  inflating: 434053.txt              \n",
            "  inflating: 436053.txt              \n",
            "  inflating: 445053.txt              \n",
            "  inflating: 447053.txt              \n",
            "  inflating: 46053.txt               \n",
            "  inflating: 497053.txt              \n",
            "  inflating: 502053.txt              \n",
            "  inflating: 511053.txt              \n",
            "  inflating: 515053.txt              \n",
            "  inflating: 516053.txt              \n",
            "  inflating: 52053.txt               \n",
            "  inflating: 521053.txt              \n",
            "  inflating: 530053.txt              \n",
            "  inflating: 536053.txt              \n",
            "  inflating: 540053.txt              \n",
            "  inflating: 541053.txt              \n",
            "  inflating: 544053.txt              \n",
            "  inflating: 556053.txt              \n",
            "  inflating: 569053.txt              \n",
            "  inflating: 57053.txt               \n",
            "  inflating: 586053.txt              \n",
            "  inflating: 59053.txt               \n",
            "  inflating: 593053.txt              \n",
            "  inflating: 606053.txt              \n",
            "  inflating: 612053.txt              \n",
            "  inflating: 614053.txt              \n",
            "  inflating: 618053.txt              \n",
            "  inflating: 62053.txt               \n",
            "  inflating: 635053.txt              \n",
            "  inflating: 646053.txt              \n",
            "  inflating: 647053.txt              \n",
            "  inflating: 665053.txt              \n",
            "  inflating: 673053.txt              \n",
            "  inflating: 683053.txt              \n",
            "  inflating: 703053.txt              \n",
            "  inflating: 72053.txt               \n",
            "  inflating: 723053.txt              \n",
            "  inflating: 732053.txt              \n",
            "  inflating: 735053.txt              \n",
            "  inflating: 739053.txt              \n",
            "  inflating: 741053.txt              \n",
            "  inflating: 75053.txt               \n",
            "  inflating: 756053.txt              \n",
            "  inflating: 757053.txt              \n",
            "  inflating: 78053.txt               \n",
            "  inflating: 800053.txt              \n",
            "  inflating: 801053.txt              \n",
            "  inflating: 802053.txt              \n",
            "  inflating: 82053.txt               \n",
            "  inflating: 88053.txt               \n",
            "  inflating: 9053.txt                \n",
            "  inflating: 93053.txt               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = '179053.txt'\n",
        "\n",
        "\n",
        "def read_data_and_preprocess(file):\n",
        "  f = open(file, 'r')\n",
        "  data = f.read()\n",
        "  sentences = tokenize.sent_tokenize(data)\n",
        "  preprocessed = []\n",
        "  for sentence in sentences:\n",
        "    #lowercase\n",
        "    sentence = sentence.lower()\n",
        "    #remove the tags\n",
        "    prep_sentence = re.sub(r\"<h>|<p>\", \" \", sentence)\n",
        "    #remove @ and &amp\n",
        "    prep_sentence = re.sub(r\"@|&amp;\", \" \", prep_sentence)\n",
        "    #remove numbers that may begin with letter and followed by /\\\n",
        "    prep_sentence = re.sub(r\"[a-z]*\\d+\\/*\", \" \", prep_sentence)\n",
        "    #remove endlines\n",
        "    prep_sentence = re.sub(r\"\\\\n\", \" \", prep_sentence)\n",
        "    preprocessed.append(prep_sentence)\n",
        "\n",
        "  documents = []\n",
        "  for document in preprocessed:\n",
        "    doc = []\n",
        "    words = tokenize.word_tokenize(document)\n",
        "    for word in words:\n",
        "      doc.append(word)\n",
        "    documents.append(doc)\n",
        "\n",
        "\n",
        "  return documents\n",
        "\n",
        "\n",
        "documents = read_data_and_preprocess(file)"
      ],
      "metadata": {
        "id": "fwOjPqqWkeSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "\n",
        "gloss_rel = lambda x: x.definition()\n",
        "example_rel = lambda x: \" \".join(x.examples())\n",
        "hyponym_rel = lambda x: \" \".join(w.definition() for w in x.hyponyms())\n",
        "meronym_rel = lambda x: \" \".join(w.definition() for w in x.member_meronyms() + \\\n",
        "                                 x.part_meronyms() + x.substance_meronyms())\n",
        "also_rel = lambda x: \" \".join(w.definition() for w in x.also_sees())\n",
        "attr_rel = lambda x: \" \".join(w.definition() for w in x.attributes())\n",
        "hypernym_rel = lambda x: \" \".join(w.definition() for w in x.hypernyms())\n",
        "\n",
        "relpairs = {wn.NOUN: [(hyponym_rel, meronym_rel), (meronym_rel, hyponym_rel),\n",
        "                      (hyponym_rel, hyponym_rel),\n",
        "                      (gloss_rel, meronym_rel), (meronym_rel, gloss_rel),\n",
        "                      (example_rel, meronym_rel), (meronym_rel, example_rel),\n",
        "                      (gloss_rel, gloss_rel)],\n",
        "            wn.ADJ: [(also_rel, gloss_rel), (gloss_rel, also_rel),\n",
        "                     (attr_rel, gloss_rel), (gloss_rel, attr_rel),\n",
        "                     (gloss_rel, gloss_rel),\n",
        "                     (example_rel, gloss_rel), (gloss_rel, example_rel),\n",
        "                     (gloss_rel, hypernym_rel), (hypernym_rel, gloss_rel)],\n",
        "            wn.VERB:[(example_rel, example_rel),\n",
        "                     (example_rel, hypernym_rel), (hypernym_rel, example_rel),\n",
        "                     (hyponym_rel, hyponym_rel),\n",
        "                     (gloss_rel, hyponym_rel), (hyponym_rel, gloss_rel),\n",
        "                     (example_rel, gloss_rel), (gloss_rel, example_rel)]}\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Helper function to preprocess text (lowercase, remove punctuation etc.)\n",
        "    \"\"\"\n",
        "    words = nltk.word_tokenize(text)\n",
        "    punctuation = string.punctuation\n",
        "    words = [word.lower() for word in words if word not in punctuation]\n",
        "    words = [word for word in words if not word in stopwords.words('english')] # ? not part of the original algorithm to remove all stopwords! (only ones at the edges of the subsequence)\n",
        "    return words\n",
        "\n",
        "def lcs(S1, S2):\n",
        "    \"\"\"\n",
        "    Helper function to compute length and offsets of longest common substring of\n",
        "    S1 and S2. Uses the classical dynamic programming algorithm.\n",
        "    \"\"\"\n",
        "    M = [[0]*(1+len(S2)) for i in range(1+len(S1))]\n",
        "    longest, x_longest, y_longest = 0, 0, 0\n",
        "    for x in range(1,1+len(S1)):\n",
        "        for y in range(1,1+len(S2)):\n",
        "            if S1[x-1] == S2[y-1]:\n",
        "                M[x][y] = M[x-1][y-1] + 1\n",
        "                if M[x][y]>longest:\n",
        "                    longest = M[x][y]\n",
        "                    x_longest = x\n",
        "                    y_longest = y\n",
        "            else:\n",
        "                M[x][y] = 0\n",
        "    return longest, x_longest - longest, y_longest - longest\n",
        "\n",
        "def score(gloss1, gloss2, normalized=False):\n",
        "    \"\"\"\n",
        "    Compute score between two glosses based on length of common substrings.\n",
        "    \"\"\"\n",
        "    gloss1 = preprocess(gloss1)\n",
        "    gloss2 = preprocess(gloss2)\n",
        "    curr_score = 0\n",
        "    longest, start1, start2, = lcs(gloss1, gloss2)\n",
        "    while longest > 0:\n",
        "        gloss1[start1 : start1 + longest] = []\n",
        "        gloss2[start2 : start2 + longest] = []\n",
        "        curr_score += longest ** 2\n",
        "        longest, start1, start2 = lcs(gloss1, gloss2)\n",
        "    if normalized and curr_score:\n",
        "      return curr_score / (len(gloss1) + len(gloss2))\n",
        "    return curr_score\n",
        "\n",
        "def relatedness(sense1, sense2, relpairs, normalized=False):\n",
        "    \"\"\"\n",
        "    Compute the relatedness of two senses (synsets) using the list of pairs of\n",
        "    relations in relpairs.\n",
        "    \"\"\"\n",
        "    return sum(score(pair[0](sense1), pair[1](sense2), normalized=normalized) # Note: normalization not explicitly part of original algorithm!\n",
        "    for pair in relpairs)\n",
        "\n",
        "def wsd(context, target, winsize, pos_tag, verbose=False, normalized=False):\n",
        "    \"\"\"\n",
        "    Find the best sense for a word in a given context.\n",
        "    Arguments:\n",
        "    context - sentence(s) we are analyzing; expected as list of strings\n",
        "    target  - string representing the word whose senses we're trying to\n",
        "              disambiguate. Target is assumed to occur once in sentence. In case\n",
        "              of multiple occurences, the first one is considered. Will throw\n",
        "              ValueError if target is not in sentence\n",
        "    winsize - size of window used for disambiguating. The algorithm will only\n",
        "              look at winsize words of the appropriate part-of-speech around the\n",
        "              target word\n",
        "    pos_tag - part of speech of target word\n",
        "    \"\"\"\n",
        "    context = list(filter(None, [wn.synsets(word, pos=pos_tag) for word in context]))\n",
        "    target_synsets = wn.synsets(target, pos=pos_tag)\n",
        "    try:\n",
        "      pos = context.index(target_synsets)\n",
        "    except ValueError:\n",
        "      return None, 0.\n",
        "\n",
        "    window = context[max(pos - winsize, 0) : pos] + \\\n",
        "             context[pos + 1 : min(pos + winsize + 1, len(context))]\n",
        "    sense_scores = [sum(sum(relatedness(sense, other_sense, relpairs[pos_tag], normalized=normalized)\n",
        "                              for other_sense in senses)\n",
        "                   for senses in window) for sense in target_synsets]\n",
        "    if verbose:\n",
        "      print(\"All scores:\")\n",
        "      for i, s in enumerate(target_synsets):\n",
        "        print(sense_scores[i], s, s.definition())\n",
        "    best_score = max(sense_scores)\n",
        "    best_index = sense_scores.index(best_score)\n",
        "    return target_synsets[best_index], best_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAUT9MImWXXP",
        "outputId": "771c2726-c3d2-426a-89ad-ea72ddb81cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tPkLeZTdaatR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the text in the selected file and disambiguate every instance of the word `hard` and `line`. Try different approaches, using both knowledge-based and corpus-based methods:\n",
        "\n",
        "- use the trained Naive Bayes classifier above\n",
        "- use Lesk algorithm's implementation in NLTK (see previous lab)\n",
        "- use Banerjee & Pedersen's extended Lesk algoritm (see previous lab, you can use the implementation there)\n"
      ],
      "metadata": {
        "id": "8iQDBppyfcSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag_wordnet(text, input):\n",
        "  part_of_speech = ['R', 'J', 'N', 'V']\n",
        "  part_of_speech_wn = [wn.ADV, wn.ADJ, wn.NOUN, wn.VERB]\n",
        "\n",
        "  pos_text = nltk.pos_tag(text)\n",
        "  \n",
        "  get_part_of_speech = []\n",
        "  for (word, pos_tag) in pos_text:\n",
        "    if (pos_tag[0] in part_of_speech) and (pos_tag[0] != 'R'):\n",
        "      index = part_of_speech.index(pos_tag[0])\n",
        "      get_part_of_speech.append((word, part_of_speech_wn[index]))\n",
        "    else:\n",
        "      get_part_of_speech.append((word, wn.NOUN))\n",
        "  \n",
        "  return get_part_of_speech[text.index(input)][1]"
      ],
      "metadata": {
        "id": "46A00oFuWl-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISES FOR WORD 'HARD'"
      ],
      "metadata": {
        "id": "54dte47xkLEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_synsets = wn.synsets('hard')\n",
        "for element in target_synsets:\n",
        "  print(element, element.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBCImBZGdRvE",
        "outputId": "a4ad8050-4822-40bc-be76-cc0c3e259716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('difficult.a.01') not easy; requiring great physical or mental effort to accomplish or comprehend or endure\n",
            "Synset('hard.a.02') dispassionate; \n",
            "Synset('hard.a.03') resisting weight or pressure\n",
            "Synset('hard.s.04') very strong or vigorous\n",
            "Synset('arduous.s.01') characterized by effort to the point of exhaustion; especially physical effort\n",
            "Synset('unvoiced.a.01') produced without vibration of the vocal cords\n",
            "Synset('hard.a.07') (of light) transmitted directly from a pointed light source\n",
            "Synset('hard.a.08') (of speech sounds); produced with the back of the tongue raised toward or touching the velum\n",
            "Synset('intemperate.s.03') given to excessive indulgence of bodily appetites especially for intoxicating liquors\n",
            "Synset('hard.s.10') being distilled rather than fermented; having a high alcoholic content\n",
            "Synset('hard.s.11') unfortunate or hard to bear\n",
            "Synset('hard.s.12') dried out\n",
            "Synset('hard.r.01') with effort or force or vigor\n",
            "Synset('hard.r.02') with firmness\n",
            "Synset('hard.r.03') earnestly or intently\n",
            "Synset('hard.r.04') causing great damage or hardship\n",
            "Synset('hard.r.05') slowly and with difficulty\n",
            "Synset('heavily.r.07') indulging excessively\n",
            "Synset('hard.r.07') into a solid condition\n",
            "Synset('hard.r.08') very near or close in space or time\n",
            "Synset('hard.r.09') with pain or distress or bitterness\n",
            "Synset('hard.r.10') to the full extent possible; all the way\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {\n",
        "    \"HARD1\": [\"difficult.a.01\", \"arduous.s.01\", \"hard.s.11\", \"hard.r.01\",\"hard.r.09\", 'hard.r.10'],    \n",
        "    \"HARD2\": [\"hard.a.02\", \"hard.s.04\", \"unvoiced.a.01\", \"intemperate.s.03\"],\n",
        "    \"HARD3\": [\"hard.a.03\", \"hard.r.07\", \"heavily.r.07\",\"hard.a.08\"], \n",
        "}"
      ],
      "metadata": {
        "id": "3y3kID3fdOnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = wsd_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_context_features)\n",
        "voc_hard = extract_vocab2(documents, \"hard\")\n",
        "print(\"-----\")\n",
        "total_occurences = 0\n",
        "total_1_2 = 0\n",
        "total_1_3 = 0\n",
        "total_2_3 = 0\n",
        "total_1_2_3 = 0\n",
        "\n",
        "for sent in documents:\n",
        "  new_document = preprocess(' '.join(sent))\n",
        "  if 'hard' in new_document:\n",
        "    total_occurences += 1\n",
        "    pos = pos_tag_wordnet(new_document, 'hard')\n",
        "    ws = wsd(context=new_document, target=\"hard\", winsize=3, pos_tag=pos)\n",
        "    extended = True\n",
        "    if ws[0] is None:\n",
        "      extended = False\n",
        "      print(\"extended Lesk: {}\".format(None))\n",
        "    else:\n",
        "      print(\"extended Lesk: {}\".format(ws[0].name()))\n",
        "    print(\"Lesk: {}\".format(lesk(new_document, 'hard').name()))\n",
        "    ws2 = wsd_context_features1(new_document, 'hard', voc_hard)\n",
        "    print(\"Classifier NB: {}\".format(classifier.classify(ws2)))\n",
        "\n",
        "    #print(mapping[classifier.classify(ws2)])\n",
        "    print(\"-----\")\n",
        "    if classifier.classify(ws2) is not None and ws[0] is not None:\n",
        "      ok = 0\n",
        "      if ws[0].name() == lesk(new_document, 'hard').name():\n",
        "        total_1_2 += 1\n",
        "        ok += 1\n",
        "      \n",
        "      if ws[0].name() in mapping[classifier.classify(ws2)]:\n",
        "        total_1_3 += 1\n",
        "        ok += 1\n",
        "      \n",
        "      if lesk(new_document, 'hard').name() in mapping[classifier.classify(ws2)]:\n",
        "        total_2_3 += 1\n",
        "        ok += 1\n",
        "      \n",
        "      if ok == 3:\n",
        "        total_1_2_3 += 1\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGJPzfkDW31m",
        "outputId": "70ea0145-7562-4a60-d5e3-2443354cfe9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8927\n",
            "-----\n",
            "extended Lesk: hard.s.04\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: arduous.s.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.a.03\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD2\n",
            "-----\n",
            "extended Lesk: hard.s.10\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.a.08\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.s.10\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.a.08\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD2\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: intemperate.s.03\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: arduous.s.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.a.08\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.a.03\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: unvoiced.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: hard.a.03\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.a.03\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: unvoiced.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: hard.a.08\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: arduous.s.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD2\n",
            "-----\n",
            "extended Lesk: arduous.s.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: arduous.s.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD2\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD2\n",
            "-----\n",
            "extended Lesk: hard.a.08\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.a.02\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: unvoiced.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: hard.a.08\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: hard.a.07\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.a.03\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: arduous.s.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: unvoiced.a.01\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.a.07\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD2\n",
            "-----\n",
            "extended Lesk: unvoiced.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: None\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD3\n",
            "-----\n",
            "extended Lesk: difficult.a.01\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n",
            "extended Lesk: hard.s.04\n",
            "Lesk: hard.s.11\n",
            "Classifier NB: HARD1\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Compute the proportion of word occurrences which were disambiguated identically by the three algorithms, separately for `line` and `hard` (#identical outputs / #total occurences of word). For which of the two words is there higher agreement between the methods? \n",
        "\n",
        "You can create your own mapping between Senseval2 and WordNet senses (doesn't need to be perfect). More info on mappings here: http://lcl.uniroma1.it/wsdeval/\n"
      ],
      "metadata": {
        "id": "7w-WZb8ogW0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Proportion 3 algorithms: {}\".format(total_1_2_3/total_occurences))\n",
        "print(\"Proportion Lesk and Extended Lesk : {}\".format(total_1_2/total_occurences))\n",
        "print(\"Proportion Lesk and NB: {}\".format(total_1_3/total_occurences))\n",
        "print(\"Proportion Extended Lesk and NB: {}\".format(total_2_3/total_occurences)) #high agreement"
      ],
      "metadata": {
        "id": "yDyvprK_gxAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd30e0e2-1a98-40b4-a634-363936e0f16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion 3 algorithms: 0.0\n",
            "Proportion Lesk and Extended Lesk : 0.0\n",
            "Proportion Lesk and NB: 0.5113636363636364\n",
            "Proportion Extended Lesk and NB: 0.6363636363636364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Pick one of the knowledge-based algorithms above and print the instances where it disagreed with the Naive Bayes method. (they returned a different prediction): show the context where the word occured, and the outputs for each of the methods."
      ],
      "metadata": {
        "id": "DDQjWMHrgxpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lesk and NB\n",
        "classifier = wsd_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_context_features)\n",
        "voc_hard = extract_vocab2(documents, \"hard\")\n",
        "print(\"-----\")\n",
        "for sent in documents:\n",
        "  new_document = preprocess(' '.join(sent))\n",
        "  if 'hard' in new_document:\n",
        "    ws2 = wsd_context_features1(new_document, 'hard', voc_hard)\n",
        "    if classifier.classify(ws2) is not None :\n",
        "      if lesk(new_document, 'hard').name() not in mapping[classifier.classify(ws2)]:\n",
        "        print(\"Document: {}\".format(' '.join(new_document)))\n",
        "        print(\"Lesk output: {}\".format(lesk(new_document, 'hard').name()))\n",
        "        print(\"NB output: {}\".format(mapping[classifier.classify(ws2)]))\n",
        "        print(\"-----\")\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "KhwRp_-dg-_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f90fb64-2cf9-4791-fb44-32bf322627c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8927\n",
            "-----\n",
            "Document: add degree confusion name suspension design gets suspension designs even r/c pan hard bar track bar helps locate axle vehicle keeps axle housing moving side side\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: 's worth noting link design properly triangulated need pan hard bar\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.02', 'hard.s.04', 'unvoiced.a.01', 'intemperate.s.03']\n",
            "-----\n",
            "Document: climb rock get back boat almost hard getting rock definitely worth effort\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: 're hard anodized use larger stronger hardware durability\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: machined sway bar clamps little hard see 've added install\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.02', 'hard.s.04', 'unvoiced.a.01', 'intemperate.s.03']\n",
            "-----\n",
            "Document: upgrading steering links aluminum improve steering response conditions especially hard binds associated rock crawling\n",
            "Lesk output: intemperate.s.03\n",
            "NB output: ['difficult.a.01', 'arduous.s.01', 'hard.s.11', 'hard.r.01', 'hard.r.09', 'hard.r.10']\n",
            "-----\n",
            "Document: kids loved dad jeep things got crazy busy able get little footage action check crouds got larger larger locals went home grab heavily modified axial rigs come run course check guys hard body jk exo bodied wraith cool\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: race n't hard enough complete let alone win\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: motul motul `` shine go `` another spray cleaner used clean rigs hard day trails\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: owner brand new jeep jk probably leave shop hard watch drill push outside first step send crusher corners powder coated\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: 's another tip -- use sharpie make body lines know sometimes hard stay exactly lines\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.02', 'hard.s.04', 'unvoiced.a.01', 'intemperate.s.03']\n",
            "-----\n",
            "Document: guys rebel road working extremely hard get rig ready action thank rebel\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: plastic steering linkage still flex enough hard crashes protect servo\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: middle area built track sherpas trophy cars need comply hard rules crawlers lexan bodies must bumpers doors scale chassis winch sand plates mounted cars realistic see\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.02', 'hard.s.04', 'unvoiced.a.01', 'intemperate.s.03']\n",
            "-----\n",
            "Document: simplest terms brushes hard conductors one positive one negative brush spinning commutator center motor positive negative magnets inside\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.02', 'hard.s.04', 'unvoiced.a.01', 'intemperate.s.03']\n",
            "-----\n",
            "Document: mike bishop crew really working hard something good honored able get firsthand look azusa canyon off-road park\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: looked long hard many companies making xj parts awesome stuff\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: drivers found hard way missing course completion less second others barely pulled\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: huge thanks goes brian parker stuart gartner hard work throughout whole series could n't done without guys\n",
            "Lesk output: unvoiced.a.01\n",
            "NB output: ['difficult.a.01', 'arduous.s.01', 'hard.s.11', 'hard.r.01', 'hard.r.09', 'hard.r.10']\n",
            "-----\n",
            "Document: making everyone crawl ground looking spacer werty tried light truck fire using hard case lipo\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: fits existing axial beadlocks allows precise easy addition weights inside wheel add oz wheel recon birthday bash still hard believe recon three years old\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.02', 'hard.s.04', 'unvoiced.a.01', 'intemperate.s.03']\n",
            "-----\n",
            "Document: sandy lead mechanic shop busy strike pose us caught natural environment austin right latest addition crew always fly really hard capture film manage catch candid rewinding winch chris help\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n",
            "Document: really n't hard\n",
            "Lesk output: hard.s.11\n",
            "NB output: ['hard.a.03', 'hard.r.07', 'heavily.r.07', 'hard.a.08']\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Train several NaiveBayes models for 'hard.pos'/'interest.pos', including at least the following: \n",
        "- for the wsd_word_features version, vary number between 100, 200 and 300, \n",
        "- and vary the stopwords_list between [] (i.e., the null list) and STOPWORDS; \n",
        "\n"
      ],
      "metadata": {
        "id": "kLqX7koJg_hA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- for the wsd_context_features version, vary the distance between 1, 2 and 3, \n",
        "- and vary the stopwords_list between [] and STOPWORDS.\n",
        "- try to only keep the POS of the words in the context (remove the word itself from the features, and use the POSs instead)"
      ],
      "metadata": {
        "id": "fAF44bSwD6Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nWSD_WORD_FEATURES\\n\")\n",
        "numbers = [100, 200, 300]\n",
        "for number in numbers:\n",
        "  print(\"\\nNumber: {}, STOPWORDS=YES\".format(number))\n",
        "  classifier = wsd_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features, number = number)\n",
        "  print(\"\\nNumber: {}, STOPWORDS=NO\".format(number))\n",
        "  classifier = wsd_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features, number = number, stopwords_list=[])\n",
        "\n",
        "\n",
        "print(\"\\nWSD_CONTEXT_FEATURES\\n\")\n",
        "distances = [1, 2, 3]\n",
        "for distance in distances:\n",
        "  print(\"\\nDistance: {}, STOPWORDS=YES\".format(distance))\n",
        "  classifier = wsd_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features, distance = distance)\n",
        "  print(\"\\nDistance: {}, STOPWORDS=NO\".format(distance))\n",
        "  classifier = wsd_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features, distance = distance, stopwords_list=[])"
      ],
      "metadata": {
        "id": "czPs98HshDIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0974a78e-5622-4d01-9760-f014d69058a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WSD_WORD_FEATURES\n",
            "\n",
            "\n",
            "Number: 100, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8431\n",
            "\n",
            "Number: 100, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8489\n",
            "\n",
            "Number: 200, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8547\n",
            "\n",
            "Number: 200, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8581\n",
            "\n",
            "Number: 300, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8547\n",
            "\n",
            "Number: 300, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8604\n",
            "\n",
            "WSD_CONTEXT_FEATURES\n",
            "\n",
            "\n",
            "Distance: 1, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8547\n",
            "\n",
            "Distance: 1, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8604\n",
            "\n",
            "Distance: 2, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8547\n",
            "\n",
            "Distance: 2, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8604\n",
            "\n",
            "Distance: 3, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8547\n",
            "\n",
            "Distance: 3, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: HARD2 HARD3 HARD1\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.8604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISES FOR WORD 'LINE'"
      ],
      "metadata": {
        "id": "AxQHMXPqkQcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_synsets = wn.synsets('line')\n",
        "for element in target_synsets:\n",
        "  print(element, element.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luBeVltfklvM",
        "outputId": "888302ee-622f-48b2-d77c-5400d38f4a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('line.n.01') a formation of people or things one beside another\n",
            "Synset('line.n.02') a mark that is long relative to its width\n",
            "Synset('line.n.03') a formation of people or things one behind another\n",
            "Synset('line.n.04') a length (straight or curved) without breadth or thickness; the trace of a moving point\n",
            "Synset('line.n.05') text consisting of a row of words written across a page or computer screen\n",
            "Synset('line.n.06') a single frequency (or very narrow band) of radiation in a spectrum\n",
            "Synset('line.n.07') a fortified position (especially one marking the most forward position of troops)\n",
            "Synset('argumentation.n.02') a course of reasoning aimed at demonstrating a truth or falsehood; the methodical process of logical reasoning\n",
            "Synset('cable.n.02') a conductor for transmitting electrical or optical signals or electric power\n",
            "Synset('course.n.02') a connected series of events or actions or developments\n",
            "Synset('line.n.11') a spatial location defined by a real or imaginary unidimensional extent\n",
            "Synset('wrinkle.n.01') a slight depression in the smoothness of a surface\n",
            "Synset('pipeline.n.02') a pipe used to transport liquids or gases\n",
            "Synset('line.n.14') the road consisting of railroad track and roadbed\n",
            "Synset('telephone_line.n.02') a telephone connection\n",
            "Synset('line.n.16') acting in conformity\n",
            "Synset('lineage.n.01') the descendants of one individual\n",
            "Synset('line.n.18') something (as a cord or rope) that is long and thin and flexible\n",
            "Synset('occupation.n.01') the principal activity in your life that you do to earn money\n",
            "Synset('line.n.20') in games or sports; a mark indicating positions or bounds of the playing area\n",
            "Synset('channel.n.05') (often plural) a means of communication or access\n",
            "Synset('line.n.22') a particular kind of product or merchandise\n",
            "Synset('line.n.23') a commercial organization serving as a common carrier\n",
            "Synset('agate_line.n.01') space for one line of print (one column wide and 1/14 inch deep) used to measure advertising\n",
            "Synset('credit_line.n.01') the maximum credit that a customer is allowed\n",
            "Synset('tune.n.01') a succession of notes forming a distinctive sequence\n",
            "Synset('line.n.27') persuasive but insincere talk that is usually intended to deceive or impress\n",
            "Synset('note.n.02') a short personal letter\n",
            "Synset('line.n.29') a conceptual separation or distinction\n",
            "Synset('production_line.n.01') mechanical system in a factory whereby an article is conveyed through sites at which successive operations are performed on it\n",
            "Synset('line.v.01') be in line with; form a line along\n",
            "Synset('line.v.02') cover the interior of\n",
            "Synset('trace.v.02') make a mark or lines on a surface\n",
            "Synset('line.v.04') mark with lines\n",
            "Synset('line.v.05') fill plentifully\n",
            "Synset('line.v.06') reinforce with fabric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = { \"cord\": [\"cable.n.02\",\"line.n.18\", \"line.n.20\"],\n",
        "    \"phone\": [\"telephone_line.n.02\"],\n",
        "    \"product\": [\"line.n.22\"],\n",
        "    \"text\": [\"line.n.05\", \"agate_line.n.01\", \"note.n.02\"],\n",
        "    \"division\": [\"line.n.29\"],\n",
        "    'formation': ['line.n.03']\n",
        "}"
      ],
      "metadata": {
        "id": "phla11xWkTY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = wsd_classifier(NaiveBayesClassifier.train, 'line.pos', wsd_context_features)\n",
        "voc_hard = extract_vocab2(documents, \"line\")\n",
        "print(\"-----\")\n",
        "total_occurences = 0\n",
        "total_1_2 = 0\n",
        "total_1_3 = 0\n",
        "total_2_3 = 0\n",
        "total_1_2_3 = 0\n",
        "\n",
        "for sent in documents:\n",
        "  new_document = preprocess(' '.join(sent))\n",
        "  if 'line' in new_document:\n",
        "    total_occurences += 1\n",
        "    pos = pos_tag_wordnet(new_document, 'line')\n",
        "    ws = wsd(context=new_document, target=\"line\", winsize=3, pos_tag=pos)\n",
        "    if ws[0] is None:\n",
        "      print(\"extended Lesk: {}\".format(None))\n",
        "    else:\n",
        "      print(\"extended Lesk: {}\".format(ws[0].name()))\n",
        "    print(\"Lesk: {}\".format(lesk(new_document, 'line').name()))\n",
        "    ws2 = wsd_context_features1(new_document, 'line', voc_hard)\n",
        "    print(\"Classifier NB: {}\".format(classifier.classify(ws2)))\n",
        "\n",
        "    print(\"-----\")\n",
        "    if classifier.classify(ws2) is not None and ws[0] is not None:\n",
        "      ok = 0\n",
        "      if ws[0].name() == lesk(new_document, 'line').name():\n",
        "        total_1_2 += 1\n",
        "        ok += 1\n",
        "      \n",
        "      if ws[0].name() in mapping[classifier.classify(ws2)]:\n",
        "        total_1_3 += 1\n",
        "        ok += 1\n",
        "      \n",
        "      if lesk(new_document, 'line').name() in mapping[classifier.classify(ws2)]:\n",
        "        total_2_3 += 1\n",
        "        ok += 1\n",
        "      \n",
        "      if ok == 3:\n",
        "        total_1_2_3 += 1\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CbK4BMXkr18",
        "outputId": "fe00d155-349c-43e3-c66e-8f5886f902a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.7470\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: trace.v.02\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: agate_line.n.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.22\n",
            "Lesk: line.v.01\n",
            "Classifier NB: product\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: product\n",
            "-----\n",
            "extended Lesk: tune.n.01\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: cable.n.02\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: division\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: occupation.n.01\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.22\n",
            "Lesk: line.v.01\n",
            "Classifier NB: product\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.03\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: production_line.n.01\n",
            "Lesk: production_line.n.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: trace.v.02\n",
            "Classifier NB: product\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: phone\n",
            "-----\n",
            "extended Lesk: tune.n.01\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: agate_line.n.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: division\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: trace.v.02\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: cable.n.02\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: agate_line.n.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: phone\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: division\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: production_line.n.01\n",
            "Lesk: production_line.n.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.02\n",
            "Lesk: agate_line.n.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: cord\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.01\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.01\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.20\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.20\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.20\n",
            "Lesk: line.v.01\n",
            "Classifier NB: cord\n",
            "-----\n",
            "extended Lesk: line.n.03\n",
            "Lesk: agate_line.n.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: tune.n.01\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: cord\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: agate_line.n.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.20\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: trace.v.02\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: agate_line.n.01\n",
            "Classifier NB: phone\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: division\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: division\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.14\n",
            "Lesk: line.v.01\n",
            "Classifier NB: cord\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: wrinkle.n.01\n",
            "Lesk: agate_line.n.01\n",
            "Classifier NB: phone\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: trace.v.02\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.02\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.22\n",
            "Lesk: line.v.01\n",
            "Classifier NB: product\n",
            "-----\n",
            "extended Lesk: line.n.20\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: product\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: telephone_line.n.02\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: division\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: agate_line.n.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: lineage.n.01\n",
            "Lesk: trace.v.02\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.01\n",
            "Lesk: line.v.01\n",
            "Classifier NB: product\n",
            "-----\n",
            "extended Lesk: line.n.22\n",
            "Lesk: line.n.22\n",
            "Classifier NB: product\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: product\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.04\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.11\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: formation\n",
            "-----\n",
            "extended Lesk: line.n.18\n",
            "Lesk: line.v.01\n",
            "Classifier NB: text\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Proportion 3 algorithms: {}\".format(total_1_2_3/total_occurences))\n",
        "print(\"Proportion Lesk and Extended Lesk : {}\".format(total_1_2/total_occurences))\n",
        "print(\"Proportion Lesk and NB: {}\".format(total_1_3/total_occurences))\n",
        "print(\"Proportion Extended Lesk and NB: {}\".format(total_2_3/total_occurences)) #high agreement"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiXn6M_FxSq5",
        "outputId": "5229d201-4630-4178-af74-a97325db9102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion 3 algorithms: 0.00847457627118644\n",
            "Proportion Lesk and Extended Lesk : 0.025423728813559324\n",
            "Proportion Lesk and NB: 0.0423728813559322\n",
            "Proportion Extended Lesk and NB: 0.06779661016949153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The proportion is larger when it comes to 'line.pos' than 'hard.pos', but, in the same time the proportions using algorithm NB and lesk/extended lesk are larger on 'hard.pos' than 'line.pos'"
      ],
      "metadata": {
        "id": "_Rx0677-x6JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lesk and NB\n",
        "classifier = wsd_classifier(NaiveBayesClassifier.train, 'line.pos', wsd_context_features)\n",
        "voc_line = extract_vocab2(documents, \"line\")\n",
        "print(\"-----\")\n",
        "for sent in documents:\n",
        "  new_document = preprocess(' '.join(sent))\n",
        "  if 'line' in new_document:\n",
        "    ws2 = wsd_context_features1(new_document, 'line', voc_line)\n",
        "    if classifier.classify(ws2) is not None :\n",
        "      if lesk(new_document, 'line').name() not in mapping[classifier.classify(ws2)]:\n",
        "        print(\"Document: {}\".format(' '.join(new_document)))\n",
        "        print(\"Lesk output: {}\".format(lesk(new_document, 'line').name()))\n",
        "        print(\"NB output: {}\".format(mapping[classifier.classify(ws2)]))\n",
        "        print(\"-----\")\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLGWS_6bxnMy",
        "outputId": "0f926d7d-dd53-43f7-b8e4-88ff44bdfd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.7470\n",
            "-----\n",
            "Document: bottom line larger pinion gear make axial vehicle faster\n",
            "Lesk output: trace.v.02\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: line on-site registration line **all on-site registration must made cash\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: bottom line cleaner gears especially pinion ridgecrest without removing transmission completely dropping transmission makes process much easier\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: want cut line check download waiver adult- and- minor\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: non-emergency problem reach on-site axial phone line contact listed\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: emergency contact info on-site axial phone line number temporary turned duration event starting thursday july th\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: highly versatileaxial changed original scope vehicle g still `` leatherman `` axial product line 's backyard fun begins\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.22']\n",
            "-----\n",
            "Document: axial taken key items seasoned product line put g along tube style chassis works expandable base platform interchangeable bodies\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.22']\n",
            "-----\n",
            "Document: also offer complete line various springs fine tune suspension needed see complete parts list bar part number\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: stock body mount holes wo n't line though\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: pro-line racing pro-line racing established new standard rc accessory performance designing launching line tires wheels emerging off-road market\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: going flip transmission driveshafts line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: may see someone pull insane line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: fitting ripsaw tires ever since posted sneak peek video new ripsaw tires 've getting lot questions fitting tires onto line trucks\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: see cut along edge bumper area highlighted hex driver cut wings used x-acto knife smooth everything reinstall bumper lights center skid see tires clear bumper ease even suspension cycles body chassis tires next laid cut line body black marker easier visualize final cut cut body little less think 'll need recheck see anything still rubbing see tire still hitting body little looks final trim done repeat last steps side body 're done ready fun\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: able check rigs people set line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: introduction yeti line back bar set\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: ensures two side upright pieces line servo\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.29']\n",
            "-----\n",
            "Document: suggestion assemble match c-hub red arrow assemble match top kingpin screw orange line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: drivin diva pushes line head\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: oh calling navigator for- little line assistance\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: vertical line drawn top pivot point c bottom pivot point\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: ls- motor custom steel mesh side panels allow clean line sight sides buggy keeping flying rocks debris\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: testing aftershock media day race day leaving starting line hurry team bypassed first pit stops issues\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: changed original scope vehicle g ridgecrestg still `` leatherman `` axial product line 's backyard crawling begins\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.22']\n",
            "-----\n",
            "Document: going run line times posting best run on- video check\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: competitors trickle line tech inspection\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: hold line driver winching safest way obstacle\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: carburetor features factory re-set line high speed needle flush needles making re-setting baseline factory specifications quick easy\n",
            "Lesk output: production_line.n.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: current bodies would n't make fair amount sense introduce radical new revamp scx- line\n",
            "Lesk output: trace.v.02\n",
            "NB output: ['line.n.22']\n",
            "-----\n",
            "Document: drivers recon always happy help drivers loaning strap winch line parts tools\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: bottom line wraithg tough nails prepared terrain\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: really cool look similar sliders come standard equipment line vehicles definitely fit theme build\n",
            "Lesk output: line.v.01\n",
            "NB output: ['telephone_line.n.02']\n",
            "-----\n",
            "Document: axial also offer complete line various springs fine tune suspension needed see complete parts list info\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: struggling installing new bearings use old outer axle line properly press place\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: rubber bands wound servo activated brake hold vehicle starting line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.29']\n",
            "-----\n",
            "Document: may seen glass cut first scored sharp device snapped score line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: score trim lines bend excess material back forth body snap apart score line\n",
            "Lesk output: trace.v.02\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: drivers course marked green axial markers option taking challenging bonus line marked orange axial markers\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: marked straight line gave lower rockers fold body tucks chassis prevent getting caught rocks\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: line holes rings holes wheels\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: based prelim times line set main event\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: jake co-dog bernie found starting th line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: race came end gary ferravanti sr. crossed line first followed jon cagliero and- jake finishing rd collecting another podium finish\n",
            "Lesk output: line.v.01\n",
            "NB output: ['telephone_line.n.02']\n",
            "-----\n",
            "Document: nothing rule book stating case tie john goodby cal rock racing decided go motocross rule states whoever crossed finish line first two points leaders would new champion\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.29']\n",
            "-----\n",
            "Document: year nd line get qualifying time\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: carburetor features factory re-set line high speed needle flush needles making re-setting baseline factory specifications quick easy\n",
            "Lesk output: production_line.n.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: extremely careful line selection needed avoid many holes possible\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: jeep also hand display complete line vehicles\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.18', 'cable.n.02']\n",
            "-----\n",
            "Document: bad line choice excessive throttle certainly result jeep lying side\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: drivers little line time\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: going shoot fromthe start line finish lineor maybe mid point mile course\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: n't even know starting line yet\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: n't long team filtering pits going car lowered onto trailer headed starting line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: still trying keep eye race car noticed everyone already loaded wasted time hot pursuit following everyone towards finish line miles away\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: wind calmed quite bit headed start line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: caught photos wanted start line raced towards six mile marker\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: waited waited nothing happenedthen saw part crew heading back towards starting line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.18', 'cable.n.02']\n",
            "-----\n",
            "Document: came line rig would fall boulders took precious minutes recover\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: jeep- wrangler become staple axial line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: thing noticed brad would always take easy line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.18', 'cable.n.02']\n",
            "-----\n",
            "Document: line started and- looks like- everyone came play\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: fastened place screws small mount parts tree http ** toolong oil cooler lines used- fuel- line nitro rc small zip ties\n",
            "Lesk output: trace.v.02\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: used one piece fuel line ran body fastening ends oil cooler\n",
            "Lesk output: agate_line.n.01\n",
            "NB output: ['telephone_line.n.02']\n",
            "-----\n",
            "Document: axial great using common gear train vehicle line hoping see gear sets exo vehicles\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: lots people already arrived line tech inspection\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.29']\n",
            "-----\n",
            "Document: look axial racing vehicle side imagine horizontal line going length vehicle\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.29']\n",
            "-----\n",
            "Document: center gravity may line depending setup\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: vehicle much weight imaginary line top heavy gravity better\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: without proper traction tough hold intended line rocks around track\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.18', 'cable.n.02']\n",
            "-----\n",
            "Document: thanks reply bender also wanted ask cgr blazer body pro line would match wheelbase truck\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: method works great careful veer intended cut line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: drill holes scribed line proper distance apart\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: keep holes line measure another door seam mirror holes\n",
            "Lesk output: agate_line.n.01\n",
            "NB output: ['telephone_line.n.02']\n",
            "-----\n",
            "Document: set proper back plate place inside body line everything carefully mark hole locations\n",
            "Lesk output: trace.v.02\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: set body place top body posts carefully line front body mounts outside body marker\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: measured inside roof line cherokee body determine width needed first cut center roll cage needed\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: made using black fuel line great planes nitro fuel line plug part cap\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: reamed hole body smaller outer diameter fuel line pressed assembly place\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: notice cut part interior away see internal roll cage pretty much sits roof line body help keep cab rigid high speed rollovers\n",
            "Lesk output: line.v.02\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: pro line recomends cut apinting opinion\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: recently 've received tremendous amount praise response product line would like show thanks appreciation giving little back\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.22']\n",
            "-----\n",
            "Document: see start line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: may remember axial popular line nitro burning engines ranging size\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.22']\n",
            "-----\n",
            "Document: tire foam pit optional bridge line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: think chose wrong line drivers\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: place sweet zip line rope swing slides diving boards catfish bridges jump parker could n't resist\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: speed n't straight line performance stability important well\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: draw imaginary line imaginary line lands close center tire achieve ideal scrub radius\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.29']\n",
            "-----\n",
            "Document: ideal zero scrub radius occurs kingpin line meets ground center tire\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: blue line imaginary line mentioned\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: screws imaginary line travels kingpin screws\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: red `` x `` bottom tire location kingpin line meets ground\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: see imaginary line kingpin screws incline axle\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: effectively moves imaginary line closer center tire\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: take look image green line running center axle helps show difference link mounts flipped link mounts shown blue\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: rigs began line noticed even halloween spirit\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: far right axials randall davis concentrating line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: right line found stock wraith ridgecrest honcho able make across ladder\n",
            "Lesk output: trace.v.02\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: hey 's waiting line behind\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.22']\n",
            "-----\n",
            "Document: bender posted remember right adjusting c-hub still n't line kingpins like wanted\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: see cut line clearly marked inside body\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: product line includes aftermarket parts axial based crawlers scale trucks desire raise performance bar\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.22']\n",
            "-----\n",
            "Document: checked rigs waiting line noticed interesting things\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: line first bog\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: extensive searching normal looking guy actually fit scale wraith ended going sam fisher action figure popular `` splinter cell `` line video games\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: last thing needed black nitro fuel line slides solder perfectly\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n",
            "Document: eric `` tree `` us- know going give line\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.03']\n",
            "-----\n",
            "Document: indeed drive line assistance\n",
            "Lesk output: line.v.01\n",
            "NB output: ['line.n.05', 'note.n.02', 'agate_line.n.01']\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nWSD_WORD_FEATURES\\n\")\n",
        "numbers = [100, 200, 300]\n",
        "for number in numbers:\n",
        "  print(\"\\nNumber: {}, STOPWORDS=YES\".format(number))\n",
        "  classifier = wsd_classifier(NaiveBayesClassifier.train, 'line.pos', wsd_word_features, number = number)\n",
        "  print(\"\\nNumber: {}, STOPWORDS=NO\".format(number))\n",
        "  classifier = wsd_classifier(NaiveBayesClassifier.train, 'line.pos', wsd_word_features, number = number, stopwords_list=[])\n",
        "\n",
        "\n",
        "print(\"\\nWSD_CONTEXT_FEATURES\\n\")\n",
        "distances = [1, 2, 3]\n",
        "for distance in distances:\n",
        "  print(\"\\nDistance: {}, STOPWORDS=YES\".format(distance))\n",
        "  classifier = wsd_classifier(NaiveBayesClassifier.train, 'line.pos', wsd_word_features, distance = distance)\n",
        "  print(\"\\nDistance: {}, STOPWORDS=NO\".format(distance))\n",
        "  classifier = wsd_classifier(NaiveBayesClassifier.train, 'line.pos', wsd_word_features, distance = distance, stopwords_list=[])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEefx6aNx0uk",
        "outputId": "8b635347-4c31-426b-b10e-e6b2d4b18f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WSD_WORD_FEATURES\n",
            "\n",
            "\n",
            "Number: 100, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6169\n",
            "\n",
            "Number: 100, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6024\n",
            "\n",
            "Number: 200, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6663\n",
            "\n",
            "Number: 200, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6578\n",
            "\n",
            "Number: 300, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6928\n",
            "\n",
            "Number: 300, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6843\n",
            "\n",
            "WSD_CONTEXT_FEATURES\n",
            "\n",
            "\n",
            "Distance: 1, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6928\n",
            "\n",
            "Distance: 1, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6843\n",
            "\n",
            "Distance: 2, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6928\n",
            "\n",
            "Distance: 2, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6843\n",
            "\n",
            "Distance: 3, STOPWORDS=YES\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6928\n",
            "\n",
            "Distance: 3, STOPWORDS=NO\n",
            "Reading data...\n",
            " Senses: cord phone product formation division text\n",
            "Training classifier...\n",
            "Testing classifier...\n",
            "Accuracy: 0.6843\n"
          ]
        }
      ]
    }
  ]
}